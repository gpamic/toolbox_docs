{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Least Squares Support Vector Classifier</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: RÃ´mulo Drumond ([GitHub](https://github.com/RomuloDrumond))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "2. [Using the classifier](#using_classifier)\n",
    "\n",
    "    2.1 [CPU/Numpy version](#cpu_version)\n",
    "    \n",
    "    2.2 [GPU/PyTorch version](#gpu_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Least-Squares Support Vector Machine (LSSVM) is a variation of the original Support Vector Machine (SVM) in which we have a slight change in the objective and restriction functions that results in a big simplification of the optimization problem.\n",
    "\n",
    "First, let's see the optimization problem of an SVM:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + C \\sum_{i=1}^{n} \\xi_i &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b)\\geq 1 - \\xi_i, && i = 1,..., n \\\\\n",
    "         && \\xi_i \\geq 0,                            && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, we have a set of inequality restrictions and when solving the optimization problem by it's dual we find a discriminative function, adding the kernel trick, of the type:\n",
    "\n",
    "\n",
    "$$ f(\\vec{x}) = sign \\ \\Big( \\sum_{i=1}^{n} \\alpha_i^o y_i K(\\vec{x}_i,\\vec{x}) + b_o \\Big) $$\n",
    "\n",
    "Where $\\alpha_i^o$ and $b_o$ denote optimum values. Giving enough regularization (smaller values of $C$) we get a lot of $\\alpha_i^o$ nulls, resulting in a sparse model in which we only need to save the pairs $(\\vec{x}_i,y_i)$ which have the optimum dual variable not null. The vectors $\\vec{x}_i$ with not null $\\alpha_i^o$ are known as support vectors (SV).\n",
    "\n",
    "\n",
    "\n",
    "In the LSSVM case, we change the inequality restrictions to equality restrictions. As the $\\xi_i$ may be negative we square its values in the objective function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + \\gamma \\frac{1}{2}\\sum_{i=1}^{n} \\xi_i^2 &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b) = 1 - \\xi_i, && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The dual of this optimization problem results in a system of linear equations, a set of Karush-Khun-Tucker (KKT) equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "    0 & \\vec{d}^T \\\\\n",
    "    \\vec{y} & \\Omega + \\gamma^{-1} I \n",
    "\\end{bmatrix}\n",
    "\\\n",
    "\\begin{bmatrix} \n",
    "    b  \\\\\n",
    "    \\vec{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    0 \\\\\n",
    "    \\vec{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where, with the kernel trick, &nbsp; $\\Omega_{i,j} = y_i y_j K(\\vec{x}_i,\\vec{x}_j)$,  &nbsp;  $\\vec{y} = [y_1 \\ y_2 \\ ... \\ y_n]^T$, &nbsp; $\\vec{\\alpha} = [\\alpha_1 \\ \\alpha_2 \\ ... \\ \\alpha_n]^T$ &nbsp;  e &nbsp; $\\vec{1} = [1 \\ 1 \\ ... \\ 1]^T$.\n",
    "\n",
    "The discriminative function of the LSSVM has the same form of the SVM but the $\\alpha_i^o$ aren't usually null, resulting in a bigger model. The big advantage of the LSSVM is in finding it's parameters, which is reduced to solving the linear system of the type:\n",
    "\n",
    "$$ A\\vec{z} = \\vec{b} $$\n",
    "\n",
    "A well-known solution of the linear system is when we minimize the square of the residues, that can be written as the optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{z})=\\frac{1}{2}||A\\vec{z} - \\vec{b}||^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And have the analytical solution:\n",
    "\n",
    "$$ \\vec{z} = A^{\\dagger} \\vec{b} $$\n",
    "\n",
    "Where $A^{\\dagger}$ is the pseudo-inverse defined as:\n",
    "\n",
    "$$ A^{\\dagger} = (A^T A)^{-1} A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using the classifier <a class=\"anchor\" id=\"using_classifier\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CPU/Numpy version <a class=\"anchor\" id=\"cpu_version\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little trick to import the toolbox from the parent folder\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from toolbox.models.svm import LSSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert dummies to multilabel\n",
    "def dummie2multilabel(X):\n",
    "    N = len(X)\n",
    "    X_multi = np.zeros((N,1),dtype='int')\n",
    "    for i in range(N):\n",
    "        temp = np.where(X[i]==1)[0] # find where 1 is found in the array\n",
    "        if temp.size == 0: # is a empty array, there is no '1' in the X[i] array\n",
    "            X_multi[i] = 0 # so we denote this class '0'\n",
    "        else:\n",
    "            X_multi[i] = temp[0] + 1\n",
    "    return X_multi.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2020)\n",
    "\n",
    "# Scaling features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_tr_norm = scaler.transform(X_train)\n",
    "X_ts_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user my find different params for each kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel:\n",
      "acc_test =  0.967741935483871 \n",
      "\n",
      "Polynomial kernel:\n",
      "acc_test =  0.9899888765294772 \n",
      "\n",
      "Linear kernel:\n",
      "acc_test =  0.978865406006674 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Gaussian kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='rbf', sigma=.5) # Class instantiation\n",
    "lssvc.fit(X_tr_norm, y_train) # Fitting the model\n",
    "y_pred = lssvc.predict(X_ts_norm) # Making preditictions with trained model\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')\n",
    "\n",
    "print('Polynomial kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='poly', d=2)\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')\n",
    "\n",
    "print('Linear kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='linear')\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in doubt, the user can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module toolbox.models.svm.LSSVC:\n",
      "\n",
      "fit(self, X, y)\n",
      "    Fits the model given the set of X attribute vectors and y labels.\n",
      "    - X: ndarray of shape (n_samples, n_attributes)\n",
      "    - y: ndarray of shape (n_samples,) or (n_samples, n)\n",
      "        If the label is represented by an array of n elements, the y \n",
      "        parameter must have n columns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LSSVC.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for a complete view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSSVC in module toolbox.models.svm.LSSVC:\n",
      "\n",
      "class LSSVC(builtins.object)\n",
      " |  LSSVC(gamma=1, kernel='rbf', **kernel_params)\n",
      " |  \n",
      " |  Class that implements the Least Squares Support Vector Machine for\n",
      " |  classification tasks\n",
      " |  \n",
      " |  It uses Numpy pseudo-inverse function to solve the dual optimization \n",
      " |  problem  with ordinary least squares. In multiclass classification problems\n",
      " |  the approach used is one-vs-all, so, a model is fit for each class while\n",
      " |  considering the others a set of the same class.\n",
      " |  \n",
      " |  # Parameters:\n",
      " |  - gamma: float, default = 1.0\n",
      " |      Constant that control the regularization of the model, it may vary \n",
      " |      in the set (0, +infinity). The closest gamma is to zero, the more \n",
      " |      regularized the model will be.\n",
      " |  - kernel: {'linear', 'poly', 'rbf'}, default = 'rbf'\n",
      " |      The kernel name utilized for the model, if set to 'linear' the model \n",
      " |      will not take advantage of the kernel trick, and the LSSVC maybe only\n",
      " |      useful for linearly separable problems.\n",
      " |  - kernel_params: **kwargs, default = depends on 'kernel' choice\n",
      " |      If kernel = 'linear', these parameters are ignored. If kernel = 'poly',\n",
      " |      'd' is accepted to set the degree of the polynomial, with default = 3. \n",
      " |      If kernel = 'rbf', 'sigma' is accepted to set the radius of the \n",
      " |      gaussian, with default = 1. \n",
      " |   \n",
      " |  # Attributes:\n",
      " |  - All hyperparameters of section \"Parameters\".\n",
      " |  - alpha: ndarray of shape (n_classes, n_support_vectors)\n",
      " |      Each column is the optimum value of the dual variable for each model\n",
      " |      (using the one-vs-all approach we have n_classes == n_classifiers), \n",
      " |      it can be seen as the weight given to the support vectors \n",
      " |      (sv_x, sv_y). As usually there is no alpha == 0, we usually have \n",
      " |      n_support_vectors == n_train_samples.\n",
      " |  - b: ndarray of shape (n_classes,)\n",
      " |      The optimum value of the bias of the model. In a problem with n \n",
      " |      classes, we will have n models and n bias.\n",
      " |  - sv_x: ndarray of shape (n_support_vectors, n_features)\n",
      " |      The set of the supporting vectors attributes, it has the shape \n",
      " |      of the training data.\n",
      " |  - sv_y: ndarray of shape (n_support_vectors, n)\n",
      " |      The set of the supporting vectors labels, it has the shape of the \n",
      " |      training data. If the label is represented by an array of n columns, \n",
      " |      the sv_y attribute will have n columns.\n",
      " |  - y_labels: ndarray of shape (n_classes, n)\n",
      " |      The set of unique labels. If the label is represented by an array \n",
      " |      of n columns, the y_label attribute will have n columns.\n",
      " |  - K: function\n",
      " |      Kernel function of the model.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, gamma=1, kernel='rbf', **kernel_params)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  dump(self, filepath='model', only_hyperparams=False)\n",
      " |      This method saves the model in a JSON format.\n",
      " |      - filepath: string, default = 'model'\n",
      " |          File path to save the model's json.\n",
      " |      - only_hyperparams: boolean, default = False\n",
      " |          To either save only the model's hyperparameters or not, it \n",
      " |          only affects trained/fitted models.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fits the model given the set of X attribute vectors and y labels.\n",
      " |      - X: ndarray of shape (n_samples, n_attributes)\n",
      " |      - y: ndarray of shape (n_samples,) or (n_samples, n)\n",
      " |          If the label is represented by an array of n elements, the y \n",
      " |          parameter must have n columns.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predicts the labels of data X given an trained model.\n",
      " |      - X: ndarray of shape (n_samples, n_attributes)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(filepath, only_hyperparams=False) from builtins.type\n",
      " |      This class method loads a model from a .json file.\n",
      " |      - filepath: string\n",
      " |          The model's .json file path.\n",
      " |      - only_hyperparams: boolean, default = False\n",
      " |          To either load only the model's hyperparameters or not, it \n",
      " |          only has effects when the dump of the model as done with the\n",
      " |          model's parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LSSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also save the model in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.978865406006674\n",
      "acc_test =  0.978865406006674\n"
     ]
    }
   ],
   "source": [
    "lssvc.dump('model')\n",
    "loaded_model = LSSVC.load('model')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(loaded_model.predict(X_ts_norm))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GPU/PyTorch version <a class=\"anchor\" id=\"gpu_version\"></a>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel:\n",
      "acc_test =  0.967741935483871 \n",
      "\n",
      "Polynomial kernel:\n",
      "acc_test =  0.9899888765294772 \n",
      "\n",
      "Linear kernel:\n",
      "acc_test =  0.978865406006674 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from toolbox.models.svm import LSSVC_GPU\n",
    "\n",
    "print('Gaussian kernel:')\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel='rbf', sigma=.5) \n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')\n",
    "\n",
    "print('Polynomial kernel:')\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel='poly', d=2)\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')\n",
    "\n",
    "print('Linear kernel:')\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel='linear')\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.978865406006674\n",
      "acc_test =  0.978865406006674\n"
     ]
    }
   ],
   "source": [
    "lssvc_gpu.dump('model')\n",
    "loaded_model = LSSVC_GPU.load('model')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(loaded_model.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can even change between CPU and GPU version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.978865406006674\n",
      "acc_test =  0.978865406006674\n"
     ]
    }
   ],
   "source": [
    "lssvc.dump('model_from_cpu')\n",
    "lssvc_gpu = LSSVC_GPU.load('model_from_cpu')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test =  0.978865406006674\n",
      "acc_test =  0.978865406006674\n"
     ]
    }
   ],
   "source": [
    "lssvc_gpu.dump('model_from_gpu')\n",
    "lssvc = LSSVC.load('model_from_gpu')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
